{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay in Aversive Environments - Localiser decoding\n",
    "\n",
    "#### _This is a template that will be parameterised and run via [Papermill](http://papermill.readthedocs.io/) for each subject_\n",
    "\n",
    "This notebook trains a classifier on the localiser data to identify the neural signature associated with each image in the task.\n",
    "\n",
    "Classification steps:\n",
    "\n",
    "1. Loading preprocessed data\n",
    "2. Hyperparameter optimisation\n",
    "3. Fitting the classifier\n",
    "4. Producing a confusion matrix to assess classifier performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'code')\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.externals import joblib\n",
    "from scipy.stats import halfcauchy\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from utils import add_features\n",
    "from plotting import plot_confusion_matrix\n",
    "from state_prediction import *\n",
    "from sliding_window_classifiers import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# DEFAULT PARAMETERS - OVERRRIDEN BY PAPERMILL EXECUTION\n",
    "session_id = '001'  # ID of the scanning session\n",
    "output_dir = 'data/derivatives'  # Where the output data should go\n",
    "n_stim = 14  # Number of stimuli\n",
    "classifier_window = [-5, 6]  # Additional timepoints to use as features\n",
    "classifier_center_idx = 9  # The center index of the classification window, post stimulus onset\n",
    "n_pca_components = [30, 60]  # Range of PCA components to try when optimising the classifier\n",
    "param_optimisation_cv = 5  # Folds of CV to use in optimisation\n",
    "classifier_regularisation = 'l1'  # Type of regularisation to use, l1 or l2\n",
    "classifier_multiclass = 'ovr'  # Type of multi-class approach to use, ovr for one-vs-the-rest or multiclass\n",
    "confusion_matrix_cv = 5  # CV to use for making the confusion matrix\n",
    "n_iter_search = 100  # Number of iterations of the random search parameter optimisation procedure\n",
    "cores = 1  # Number of cores to use for parallel processing\n",
    "os.environ['OMP_NUM_THREADS'] = str(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localiser_epochs = mne.read_epochs(os.path.join(output_dir, 'preprocessing/localiser', 'sub-{0}_ses-01_task-AversiveLearningReplay_run-localiser_proc_ICA-epo.fif.gz').format(session_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the responses to image stimuli in sensor space\n",
    "\n",
    "We should see an occipital-focused response from around 100ms onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.arange(0.06, 0.3, 0.02)\n",
    "evoked = localiser_epochs.average()\n",
    "evoked.plot_topomap(times, ch_type='mag')\n",
    "evoked.plot_topomap(0.2, ch_type='mag', show_names=True, colorbar=False, size=3, res=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise hyperparameters using randomised search\n",
    "\n",
    "Optimising regularisation parameter (C) and number of PCA components. Randomised search works like grid search but rather than exhaustively searching a grid of predefined parameter values, it samples from specified parameter distributions. This is useful here because C values closer to 0 tend to be better, but this is not always the case - here we sample C values from a half-Cauchy distribution so that low values are tested more frequently, without us having to manually specify a grid that conforms to this criterion.\n",
    "\n",
    "To make the process more streamlined, we create a classifier pipeline containing the following steps:\n",
    "1. Temporal PCA (reducing dimensionality in the channel dimension)\n",
    "2. Adding features from adjacent timepoints - although we're focusing on a particular timepoint, we add timepoints from before and after this point as additional features. This tends to boost decoding accuracy by ~10%.\n",
    "3. Scaling the data to be in a standard range.\n",
    "4. Logistic regression with regularisation and multi-class classification.\n",
    "\n",
    "This is the iteratively run and evaluated with cross validation across different hyperparameter settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get epoch data\n",
    "X_raw = localiser_epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times (exclude non MEG channels)\n",
    "y_raw = localiser_epochs.events[:, 2]  # Get event types\n",
    "\n",
    "# select events and time period of interest\n",
    "picks_meg = mne.pick_types(localiser_epochs.info, meg=True, ref_meg=False)\n",
    "event_selector = (y_raw < n_stim * 2 + 1)\n",
    "X_raw = X_raw[event_selector, ...]\n",
    "y_raw = y_raw[event_selector]\n",
    "X_raw = X_raw[:, picks_meg, :]\n",
    "\n",
    "assert len(np.unique(y_raw)) == n_stim, \"Found {0} stimuli, expected {1}\".format(len(np.unique(y_raw)), n_stim)\n",
    "\n",
    "print(\"Number of unique events = {0}\\n\\nEvent types = {1}\".format(len(np.unique(y_raw)),\n",
    "                                                                  np.unique(y_raw)))\n",
    "\n",
    "times = localiser_epochs.times\n",
    "\n",
    "prestim_samples = int(np.abs(localiser_epochs.tmin * localiser_epochs.info['sfreq']))\n",
    "classifier_center_idx = prestim_samples + classifier_center_idx\n",
    "\n",
    "\n",
    "# Get data\n",
    "X, y = (X_raw.copy(), y_raw.copy())\n",
    "X = X[..., classifier_center_idx + classifier_window[0]:classifier_center_idx + classifier_window[1]] \n",
    "\n",
    "# Create null data\n",
    "X_null = np.zeros((X.shape[0], 272, np.sum(np.abs(classifier_window))))\n",
    "for n, i in enumerate(np.random.randint(np.sum(np.abs(classifier_window)), prestim_samples, X.shape[0])):\n",
    "    X_null[n, :, :] = X_raw[n, :, i:np.sum(np.abs(classifier_window)) + i]\n",
    "y_null = np.ones(X_null.shape[0]) * 99\n",
    "X = np.vstack([X, X_null])\n",
    "y = np.hstack([y, y_null])\n",
    "\n",
    "# Create a pipiline that combines PCA, feature augmentation, scaling, and the logistic regression classifier\n",
    "clf = make_pipeline(UnsupervisedSpatialFilter(PCA(50), average=False), \n",
    "                    FunctionTransformer(add_features, validate=False), StandardScaler(), \n",
    "                    LogisticRegression(multi_class=classifier_multiclass, C=0.1, penalty=classifier_regularisation, solver='saga', max_iter=100000, tol=0.2, class_weight=\"balanced\"))\n",
    "\n",
    "# Parameter distributions passed to the random search procedure\n",
    "param_dist = {\"unsupervisedspatialfilter__estimator__n_components\": range(*n_pca_components),\n",
    "              \"logisticregression__C\": halfcauchy(scale=5)}\n",
    "\n",
    "# run randomized search\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=param_optimisation_cv, n_jobs=8, scoring='accuracy', verbose=True)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Produce a dataframe of the search results\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "print(\"Parameter optimisation done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show the results of the optimisation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of hyperparameter optimisation\n",
    "\n",
    "We can plot the results of the randomised search on a 3D mesh, with the two optimised parameters on the X and Y axes and accuracy on the Z axis. This is produced using [plotly](http://plot.ly/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)\n",
    "\n",
    "trace = go.Mesh3d(x=results.param_logisticregression__C,\n",
    "                  y=results.param_unsupervisedspatialfilter__estimator__n_components,\n",
    "                  z=results.mean_test_score, \n",
    "                  color='#275fb5', opacity=0.20)\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Hyperparameter optimisation results',\n",
    "    autosize=True,\n",
    "    width=700,\n",
    "    height=700,\n",
    "    scene = dict(\n",
    "    xaxis = dict(\n",
    "        title='Logistic regression C'),\n",
    "    yaxis = dict(\n",
    "        title='PCA N components'),\n",
    "    zaxis = dict(\n",
    "        title='Mean accuracy'),)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make confusion matrix with 5-fold CV\n",
    "\n",
    "The confusion matrix gives us an idea of whether any individual stimuli are being poorly decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.set_params(**random_search.best_params_)\n",
    "\n",
    "# Get predictions with 5 fold CV\n",
    "y_pred = cross_val_predict(clf, X, y, cv=confusion_matrix_cv)\n",
    "mean_conf_mat = confusion_matrix(y, y_pred)\n",
    "mean_accuracy = accuracy_score(y[y != 99], y_pred[y != 99])\n",
    "mean_conf_mat = mean_conf_mat.astype('float') / mean_conf_mat.sum(axis=1)  # normalise\n",
    "\n",
    "print(\"Mean accuracy = {0}\".format(mean_accuracy))\n",
    "    \n",
    "# Plot mean confusion matrix\n",
    "plot_confusion_matrix(mean_conf_mat[:n_stim, :n_stim], title='Normalised confusion matrix, accuracy = {0}'.format(np.round(mean_accuracy, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save components of the analysis for later use\n",
    "\n",
    "First save the classifier that was fit to all the localiser data using the best hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(output_dir, 'classifier', 'classifier_idx_{0}'.format(classifier_center_idx))):\n",
    "    os.makedirs(os.path.join(output_dir, 'classifier', 'classifier_idx_{0}'.format(classifier_center_idx)))\n",
    "joblib.dump(random_search.best_estimator_ , os.path.join(output_dir, 'classifier', \n",
    "                                                         'classifier_idx_{0}'.format(classifier_center_idx), 'sub-{0}_classifier_idx_{1}.pkl').format(session_id, classifier_center_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save certain details, such as the mean accuracy, so we can analyse them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = {\n",
    "    'mean_accuracy': mean_accuracy,\n",
    "    'best_C': random_search.best_params_['logisticregression__C'],\n",
    "    'best_n_components': random_search.best_params_['unsupervisedspatialfilter__estimator__n_components']\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'classifier', 'sub-{0}_classifier_info.json'), 'w') as f:\n",
    "    json.dump(accuracy_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's helpful to save some data related to classifier performance to create group-level measures of decoding accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'classifier_idx_{0}'.format(classifier_center_idx))):\n",
    "    os.makedirs(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'classifier_idx_{0}'.format(classifier_center_idx)))\n",
    "np.save(os.path.join(output_dir, 'localiser_classifier_performance', 'confusion_matrix', 'sub-{0}_confusion_matrix_idx_{1}.pkl').format(session_id, classifier_center_idx), mean_conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "psychopy3"
  },
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nteract": {
   "version": "0.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}